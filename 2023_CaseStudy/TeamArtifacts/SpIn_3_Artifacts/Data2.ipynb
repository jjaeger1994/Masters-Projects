{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Form Data into Postgres (Data2.ipynb)\n",
    "\n",
    "The purpose of this workbook is to build on the previous data sources that were being uploaded.  In this instance, we're taking the form data from the v1 V1FEB23 table and am creating tables.  As part of this work, we're also creating .csv files which are stored within our `/dsa/groups/casestudy2023su/team03` directory.\n",
    "\n",
    "#### 1. [Inital Connection to the database](#data_connect)\n",
    "#### 2. [CSV Creation of patient forms](#data_creation)\n",
    "#### 3. [Create tables and insert data](#data_insert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_connect\"></a>Initial connection to the database\n",
    "Simlar to the Data1 Notebook, we're connecting to the database after importing all the necessary libraries. \n",
    "\n",
    "As an important note, we're connecting as a specific user.\n",
    "1. Username: dtfp3\n",
    "2. Database: pgsql.dsa.lan/casestdysu23t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for this notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import binascii\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import getpass\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "##Connect to Postgres\n",
    "user = \"dtfp3\"\n",
    "host = \"pgsql.dsa.lan\"\n",
    "database = \"casestdysu23t03\"\n",
    "password = getpass.getpass()\n",
    "connectionstring = \"postgresql://\" + user + \":\" + password + \"@\" + host + \"/\" + database\n",
    "engine = sqlalchemy.create_engine(connectionstring)\n",
    "connection = None\n",
    "schema = \"public\"\n",
    "\n",
    "try:\n",
    "    connection = engine.connect()\n",
    "except Exception as err:\n",
    "    print(\"An error has occurred trying to connect: {}\".format(err))\n",
    "\n",
    "del password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_creation\"></a>CSV Creation of Patient Forms\n",
    "We're going to be creating tables based on patient forms within the **v1feb23_raw** table. First, we need to make sure we're selecting the specific columns of data using regex.  With our **list_of_forms** variable, we're currently isolating it to just these forms: \n",
    "- DH - Diet History\n",
    "- FF - Fracture History\n",
    "- FV - Functional Vision\n",
    "- GI - General Information\n",
    "- GS - Grip Strength\n",
    "- HW - Height, Weight, and Pulse\n",
    "- MH - Medical History\n",
    "- MU - Medication Use\n",
    "- NF - Neruomuscular Function\n",
    "- TU - Tabacco & Alcohol Use\n",
    "- NP - Nottingham Power Rig\n",
    "\n",
    "While the dataset does offer these additional forms because of lack of data, irrelevance, or lack of correlation indiciated by literature:\n",
    "- PS - Prostate Health\n",
    "- PA - Physical Activity\n",
    "- QL - Lifestyle\n",
    "- BH - Back and Joint Health\n",
    "- TB - Trail Making Task B\n",
    "- TM - Teng Mini-Mental\n",
    "- SC - Specimen Collection\n",
    "- DX - Bone Density Form\n",
    "- XR - X-Ray Form\n",
    "\n",
    "In order to make the data more readable in the **General Information** form, we are also converting several numerical values to textual through dictionary mapping. This sort of process is also being handled independently depending on the form data as part of our EDA steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After opening the session, query the table\n",
    "query = \"SELECT * FROM public.v1feb23_raw\"\n",
    "v1feb23_df = pd.read_sql_query(query, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of the various forms \n",
    "list_of_forms = [\"DH\",\"FF\",\"FV\",\"GI\",\"GS\",\"HW\",\"MH\",\"MU\",\"NF\",\"TU\",\"NP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionaries to convert form integers into corresponding categories \n",
    "ERACE_dict = {1:\"1. WHITE\",2:\"2. AFRICAN AMERICAN\",3:\"3. ASIAN\",4:\"4. HISPANIC\",5:\"5. OTHER\"}\n",
    "SOC_dict = {11:\"11. Management\", 13:\"13. Business and Financial\", 15:\"15. Computer and Mathematical\", 17:\"17. Architecture and Engineering\",\\\n",
    "           19:\"19. Life; Physical; and Social Science\",21:\"21. Community and Social Service\",23:\"23. Legal\",\\\n",
    "            25:\"25. Education; Training and Library\",27:\"27. Arts; Design; Entertainment; Sports and Media\",\\\n",
    "            29:\"29. Healthcare Practitioners and Technical\" ,31:\"31. Healthcare Support\",33:\"33. Protective Service\",\\\n",
    "            35:\"35. Food Preparation and Serving Related\",37:\"37. Building and Grounds Cleaning and Maintenance\",\\\n",
    "            39:\"39. Personal Care and Service\",41:\"41. Sales and Related\",43:\"43. Office and Administrative Support\",\\\n",
    "            45:\"45. Farming; Fishing and Forestry\",47:\"47. Construction and Extraction\",49:\"49. Installation; Maintenance and Repair\"\\\n",
    "            ,51:\"51. Production\",53:\"53. Transportation and Material Moving\",55:\"55. Military Specific\"}\n",
    "edu_dict = {1:\"1. Some Elementary\",2:\"2. Elementary\",3:\"3. Some Highschool\",4:\"4. High School\",5:\"5. Some College\",6:\"6. College\",7:\"7. Some Grad\",8:\"8. Grad School\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v1_form_DH.csv', 'v1_form_FF.csv', 'v1_form_FV.csv', 'v1_form_GI.csv', 'v1_form_GS.csv', 'v1_form_HW.csv', 'v1_form_MH.csv', 'v1_form_MU.csv', 'v1_form_NF.csv', 'v1_form_TU.csv', 'v1_form_NP.csv']\n"
     ]
    }
   ],
   "source": [
    "for form in list_of_forms:\n",
    "    form_df = v1feb23_df.filter(regex=f\"^(ID)|(^{form})\")\n",
    "    if form == \"GI\":\n",
    "        form_df.GISOC = form_df.GISOC.map(SOC_dict)\n",
    "        form_df.GIEDUC = form_df.GIEDUC.map(edu_dict)\n",
    "        form_df.GIERACE = form_df.GIERACE.map(ERACE_dict)\n",
    "    form_df.to_csv(f\"/dsa/groups/casestudy2023su/team03/v1_form_{form}.csv\",index=False)\n",
    "\n",
    "# Directory that holds the form csvs\n",
    "directory = f\"/dsa/groups/casestudy2023su/team03/\"\n",
    "\n",
    "# Get all files within the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Retrieve ONlY the specified csv files \n",
    "csv_files = [file for file in files if \"_form_\" in file and file.endswith(\".csv\")]\n",
    "\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_insert\"></a>Create tables and insert data\n",
    "This process is nearly identical to what's performed in Data1.  The difference is that instead of using the SAS7DAT files, we're now loading in the CSV.  For consistency, the below details outline the process: \n",
    "\n",
    "After the files have been converted, we then need to start our import process.  An important first step is understanding whether the tables already exist or not.  Since we're starting from the raw data, we're:\n",
    "1. **SELECT** to determine whether the tables exist or not\n",
    "    1. If it does exist, the table will be **DROP**ed\n",
    "1. **CREATE** the table - we're creating these tables with a few considerations\n",
    "    1. The table will be created a concatenated string equaling filename+<_raw>\n",
    "    2. We'll use the datatypes from the CSV so everyting isn't casted as a string\n",
    "    3. Postgres has a limit of 1600 columns so if it's greater than that, we store it as a single column that can be parsed later\n",
    "1. **GRANT** all **PRIVILEGES** to users within the **PUBLIC** group so everyone can manipulate the tables \n",
    "1. **INSERT** the data in to the respective columns\n",
    "       \n",
    "As a note, it's possible for a user to have a table-lock which would prevent the tables from being dropped. This can be resolved by checking who has the active transaction and then terminating or cancelling it from either the psql terminal or it could be done through another notebook connection:\n",
    "\n",
    "**Identify the table lock** <br>\n",
    "`select datname, pid, usename, application_name, state, query_start  from pg_stat_activity where datname = 'casestdysu23t03';`\n",
    "\n",
    "**Terminate the PID** <br>\n",
    "`SELECT pg_terminate_backend(23174);`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "Table already exists - blowing it away: v1_form_dh\n",
      "Creating Table: v1_form_dh\n",
      "Table public.v1_form_dh created successfully.\n",
      "File v1_form_DH.csv inserted successfully into table public.v1_form_dh.\n",
      "Table already exists - blowing it away: v1_form_ff\n",
      "Creating Table: v1_form_ff\n",
      "Table public.v1_form_ff created successfully.\n",
      "File v1_form_FF.csv inserted successfully into table public.v1_form_ff.\n",
      "Table already exists - blowing it away: v1_form_fv\n",
      "Creating Table: v1_form_fv\n",
      "Table public.v1_form_fv created successfully.\n",
      "File v1_form_FV.csv inserted successfully into table public.v1_form_fv.\n",
      "Table already exists - blowing it away: v1_form_gi\n",
      "Creating Table: v1_form_gi\n",
      "Table public.v1_form_gi created successfully.\n",
      "File v1_form_GI.csv inserted successfully into table public.v1_form_gi.\n",
      "Table already exists - blowing it away: v1_form_gs\n",
      "Creating Table: v1_form_gs\n",
      "Table public.v1_form_gs created successfully.\n",
      "File v1_form_GS.csv inserted successfully into table public.v1_form_gs.\n",
      "Table already exists - blowing it away: v1_form_hw\n",
      "Creating Table: v1_form_hw\n",
      "Table public.v1_form_hw created successfully.\n",
      "File v1_form_HW.csv inserted successfully into table public.v1_form_hw.\n",
      "Table already exists - blowing it away: v1_form_mh\n",
      "Creating Table: v1_form_mh\n",
      "Table public.v1_form_mh created successfully.\n",
      "File v1_form_MH.csv inserted successfully into table public.v1_form_mh.\n",
      "Table already exists - blowing it away: v1_form_mu\n",
      "Creating Table: v1_form_mu\n",
      "Table public.v1_form_mu created successfully.\n",
      "File v1_form_MU.csv inserted successfully into table public.v1_form_mu.\n",
      "Table already exists - blowing it away: v1_form_nf\n",
      "Creating Table: v1_form_nf\n",
      "Table public.v1_form_nf created successfully.\n",
      "File v1_form_NF.csv inserted successfully into table public.v1_form_nf.\n",
      "Table already exists - blowing it away: v1_form_tu\n",
      "Creating Table: v1_form_tu\n",
      "Table public.v1_form_tu created successfully.\n",
      "File v1_form_TU.csv inserted successfully into table public.v1_form_tu.\n",
      "Table already exists - blowing it away: v1_form_np\n",
      "Creating Table: v1_form_np\n",
      "Table public.v1_form_np created successfully.\n",
      "File v1_form_NP.csv inserted successfully into table public.v1_form_np.\n",
      "All files inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "user = \"dtfp3\"\n",
    "host = \"pgsql.dsa.lan\"\n",
    "database = \"casestdysu23t03\"\n",
    "password = getpass.getpass()\n",
    "schema = \"public\" # Started with a different schema and then upated to public\n",
    "\n",
    "dtype2SQL = {'object' : 'TEXT', 'float64' : 'REAL', 'int64' : \"INTEGER\",\"datetime64[ns]\":'TEXT'}\n",
    "\n",
    "# Connection setup\n",
    "connection = None\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(user=user, host=host, database=database, password=password)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(directory+csv_file)\n",
    "\n",
    "        # We're dealing with the raw data that could be very messy - starting with _raw for clarity\n",
    "        table_name = csv_file.split(\".\")[0].lower()\n",
    "\n",
    "        # Check if table exists\n",
    "        check_table_query = f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '{schema}' AND table_name = '{table_name}')\"\n",
    "        cursor.execute(check_table_query)\n",
    "        table_exists = cursor.fetchone()[0]\n",
    "\n",
    "        if table_exists:\n",
    "            print(\"Table already exists - blowing it away: {}\".format(table_name))\n",
    "            # Drop the table if it exists\n",
    "            drop_table_query = f\"DROP TABLE {schema}.{table_name}\"\n",
    "            cursor.execute(drop_table_query)\n",
    "            connection.commit()\n",
    "\n",
    "        # Create the table\n",
    "        print(\"Creating Table: {}\".format(table_name))\n",
    "        \n",
    "        if len(df.columns) >= 1600:\n",
    "            # If the number of columns is equal to or more than 1600, create a table with a single \"data_column\"\n",
    "            create_table_query = f\"CREATE TABLE {schema}.{table_name} (data_column text)\"\n",
    "            cursor.execute(create_table_query)\n",
    "        else:\n",
    "            # If the number of columns is less than 1600, create a table with all the columns from the CSV\n",
    "            columns = ', '.join([f'\"{col}\" {dtype2SQL[str(df[col].dtype)]}' for col in df.columns])\n",
    "            create_table_query = f\"CREATE TABLE {schema}.{table_name} ({columns})\"\n",
    "            cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "\n",
    "        # Grant necessary privileges to all users\n",
    "        grant_query = f\"GRANT ALL PRIVILEGES ON TABLE {schema}.{table_name} TO PUBLIC\"\n",
    "        cursor.execute(grant_query)\n",
    "        connection.commit()\n",
    "        \n",
    "        print(f\"Table {schema}.{table_name} created successfully.\")\n",
    "\n",
    "        # Read in the CSV file and insert the data\n",
    "        with open(directory+csv_file, 'r') as file:\n",
    "            if len(df.columns) >= 1600:\n",
    "                # We don't skip the header because we need that data if we're building another table. \n",
    "                # If the number of columns is equal to or more than 1600, read in each row but don't parse the columns\n",
    "                for line in file:\n",
    "                    data_column = line.strip()\n",
    "                    insert_query = f\"INSERT INTO {schema}.{table_name} (data_column) VALUES (%s)\"\n",
    "                    cursor.execute(insert_query, (data_column,))\n",
    "            else:\n",
    "                # Skip header row\n",
    "                next(file)\n",
    "                \n",
    "                # If the number of columns is less than 1600, insert the data row by row\n",
    "                for line in file:\n",
    "                    values = line.strip().split(',')\n",
    "                    values = [None if x == \"\" else x for x in values]\n",
    "                    insert_query = f\"INSERT INTO {schema}.{table_name} VALUES ({','.join(['%s']*len(values))})\"\n",
    "                    cursor.execute(insert_query, values)\n",
    "\n",
    "        connection.commit()\n",
    "\n",
    "        print(f\"File {csv_file} inserted successfully into table {schema}.{table_name}.\")\n",
    "\n",
    "    print(\"All files inserted successfully.\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"An error has occurred: {}\".format(err))\n",
    "\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "del password"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
