{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpIn 5 - StartHere -- Notebook workflow/pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook indexes the workflow/pipeline for SpIn #5\n",
    "\n",
    "This week was primarily about modeling but it did start with one final push for standardization to our merged dataset.\n",
    "\n",
    "### Data Merging and Carpentry\n",
    "This notebook merges the data into a final data set for use in our predictive modeling. \n",
    "1. [Data4.ipynb](Data4.ipynb)\n",
    "    1. Reads all CSV form data, merges them based on the patient 'ID' column, then performs basic carpentry operations\n",
    "    1. The target variable is transformed into a binary response by creating a new column 'FAFXNT_BIN'\n",
    "    1. It splits the data into independent variables (X) and the target variable (y) and performs one-hot encoding on categorical features.\n",
    "    1. The data is further split into training, testing, and validation sets using train_test_split.\n",
    "    1. Principal Component Analysis (PCA) is performed on the BMD values to reduce their dimensionality.\n",
    "    1. Correlation feature reduction is applied to remove highly correlated features based on a specified threshold.\n",
    "    1. The SMOTE algorithm is used to perform oversampling on the training data to address class imbalance.\n",
    "    1. The cleaned and preprocessed datasets are saved as CSV files for further analysis and collaboration.\n",
    "\n",
    "### Model Cleanup \n",
    "As the models have developed, there was some work to review and add work to last weeks SpIn\n",
    "1. [Light GBM](EDA5_Feature_Exploration_with_LightGBM-Updated_Scoring.ipynb)\n",
    "\n",
    "\n",
    "### Modeling\n",
    "We continued our divide and conquer efforts on modeling this week. We discovered that the performance of the models drop after being tested by the validation set. Work will have to be done on further optimization, as well as analysis of what potential objectives the model may still be useful. Over the coming week, we will explore additional dimensionality reduction techniques to address potentially overfitting and/or incorporate other datasets that could provide additional information.\n",
    "\n",
    "1. [Random Forest](SPIN5_Random_Forest_Boruta.ipynb) <br>\n",
    "  <img align=\"left\" width=\"400\" height=\"300\" src=\"Random_Forest_val_performance.png\">\n",
    "<br><br><br><br><br><br><br><br>\n",
    "1. [LightGBM](EDA5_Hyperparameter_Tuning_LightGBM.ipynb) <br>\n",
    "  <img align=\"left\" width=\"400\" height=\"300\" src=\"lightgmb_val_performance.png\">\n",
    "<br><br><br><br><br><br><br><br>\n",
    "1.  [LightGBM Updated](EDA5_Hyperparameter_Tuning_LightGBM-Updated_Performance.ipynb) <br>\n",
    "   <img align=\"left\" width=\"400\" height=\"300\" src=\"Light_GBM_Updated_Performance.png\">\n",
    "<br><br><br><br><br><br><br><br>\n",
    "1. [XGBoost Model 1](XGBoostSPIN5.ipynb) <br>\n",
    "  <img align=\"left\" width=\"400\" height=\"300\" src=\"xgboost_model1_val_performance.png\">\n",
    "<br><br><br><br><br><br><br><br>\n",
    "1. [XGBoost Model 2](XGBoostSPIN5_Model2.ipynb) <br>\n",
    "  <img align=\"left\" width=\"400\" height=\"300\" src=\"xgboost_model2_val_performance.png\">\n",
    "<br><br><br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
