{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Data to Postgres (Data1.ipynb)\n",
    "\n",
    "The purpose of this workbook is take our original data sources and save them within our Postgres database.  By having the data centralized in our Postgres database, we are able to facilitate collaboration.  In order to achieve this, we have the following high-level steps:\n",
    "#### 1. [Confirm connectivity to our team database](#data_connect)\n",
    "#### 2. [Importing raw data in to the the Postgres database and schema](#data_import)\n",
    "#### 3. [Verify successful insert](#data_verify)\n",
    "\n",
    "We also have some initial exploration in to working with [wide data](#data_wide) (1600+ columns) and how we can break that in to smaller tables within Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_connect\"></a>Confirm connectivity to our team database\n",
    "Since multiple team members have had previous experience with Python, we decided to proceed with using that for our project.\n",
    "\n",
    "Through the use psycopg2, sqlalchemy, and getpass we can securely test our connection to the database.  The below code is used to verify that a specific username is able to access a specific database.\n",
    "1. Username: dtfp3\n",
    "2. Database: pgsql.dsa.lan/casestdysu23t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries for this notebook\n",
    "import pandas as pd\n",
    "import binascii\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "Successfully connected to pgsql.dsa.lan/casestdysu23t03 as dtfp3\n"
     ]
    }
   ],
   "source": [
    "# Since we were able to successfully access, I'm commenting it out\n",
    "user = \"dtfp3\" # Make sure your username is correct \n",
    "host = \"pgsql.dsa.lan\"\n",
    "database = \"casestdysu23t03\"\n",
    "password = getpass.getpass()\n",
    "connectionstring = \"postgresql://\" + user + \":\" + password + \"@\" + host + \"/\" + database\n",
    "engine = sqlalchemy.create_engine(connectionstring)\n",
    "connection = None\n",
    "try:\n",
    "    connection = engine.connect()\n",
    "    print(\"Successfully connected to {}/{} as {}\".format(host, database,user))\n",
    "except Exception as err:\n",
    "    print(\"An error has occurred trying to connect: {}\".format(err))\n",
    "    \n",
    "del password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_import\"></a>Importing raw data in to the the Postgres database and schema\n",
    "Our primary data source is from [The Osteoporotic Fractures in Men (MrOS) Study](https://mrosonline.ucsf.edu/).  After signing up for a free account, we were then able to download the raw data files.  We have stored these files in our DSA file directory:  **/dsa/groups/casestudy2023su/team03**\n",
    "\n",
    "\n",
    "Since the raw data files are stored as SAS7BDAT, many different methods require the additional step of converting them to csv for readability and import.  <br>\n",
    "`df = pd.read_sas(sas_filename)\n",
    "binary2StringLiteral(df)\n",
    "df.to_csv(csv_filename, index=False)`\n",
    "    \n",
    "After the files have been converted, we then need to start our import process.  An important first step is understanding whether the tables already exist or not.  Since we're starting from the raw data, we're:\n",
    "1. **SELECT** to determine whether the tables exist or not\n",
    "    1. If it does exist, the table will be **DROP**ed\n",
    "1. **CREATE** the table - we're creating these tables with a few considerations\n",
    "    1. The table will be created a concatenated string equaling filename+<_raw>\n",
    "    2. We'll use the datatypes from the CSV so everyting isn't casted as a string\n",
    "    3. Postgres has a limit of 1600 columns so if it's greater than that, we store it as a single column that can be parsed later\n",
    "1. **GRANT** all **PRIVILEGES** to users within the **PUBLIC** group so everyone can manipulate the tables \n",
    "1. **INSERT** the data in to the respective columns\n",
    "       \n",
    "As a note, it's possible for a user to have a table-lock which would prevent the tables from being dropped. This can be resolved by checking who has the active transaction and then terminating or cancelling it from either the psql terminal or it could be done through another notebook connection:\n",
    "\n",
    "**Identify the table lock** <br>\n",
    "`select datname, pid, usename, application_name, state, query_start  from pg_stat_activity where datname = 'casestdysu23t03';`\n",
    "\n",
    "**Terminate the PID** <br>\n",
    "`SELECT pg_terminate_backend(23174);`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n",
      "Table already exists - blowing it away: b1aug16_raw\n",
      "Creating Table: b1aug16_raw\n",
      "Table public.b1aug16_raw created successfully.\n",
      "File B1AUG16.SAS7BDAT inserted successfully into table public.b1aug16_raw.\n",
      "Table already exists - blowing it away: fafeb23_raw\n",
      "Creating Table: fafeb23_raw\n",
      "Table public.fafeb23_raw created successfully.\n",
      "File FAFEB23.SAS7BDAT inserted successfully into table public.fafeb23_raw.\n",
      "Table already exists - blowing it away: v1feb23_raw\n",
      "Creating Table: v1feb23_raw\n",
      "Table public.v1feb23_raw created successfully.\n",
      "File V1FEB23.SAS7BDAT inserted successfully into table public.v1feb23_raw.\n",
      "Creating Table: b2aug16_raw\n",
      "Table public.b2aug16_raw created successfully.\n",
      "File B2AUG16.SAS7BDAT inserted successfully into table public.b2aug16_raw.\n",
      "All files inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Alright, now for the fun stuff... \n",
    "user = \"dtfp3\"\n",
    "host = \"pgsql.dsa.lan\"\n",
    "database = \"casestdysu23t03\"\n",
    "password = getpass.getpass()\n",
    "schema = \"public\" # Started with a different schema and then upated to public\n",
    "\n",
    "# List of SAS files that the team will be uploading\n",
    "sas_files = [\n",
    "    #\"ASGAUG14.SAS7BDAT\",\n",
    "    \"B1AUG16.SAS7BDAT\",\n",
    "    \"FAFEB23.SAS7BDAT\",\n",
    "    #\"GNAUG15.SAS7BDAT\",\n",
    "    \"V1FEB23.SAS7BDAT\",\n",
    "    \"B2AUG16.SAS7BDAT\" # Adding B2AGU16 data on 7/4/23\n",
    "]\n",
    "\n",
    "dtype2SQL = {'object' : 'TEXT', 'float64' : 'REAL', 'int64' : \"INTEGER\",\"datetime64[ns]\":'TEXT'}\n",
    "##dtype2format = {'object' : r\"%s\", 'float64' : r'%', 'int64' : r\"%i\",\"datetime64[ns]\":r'%s'}\n",
    "\n",
    "# Function to convert binary data to string literals since it's possible we'll encounter it within the files\n",
    "def binary2StringLiteral(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == \"object\":\n",
    "            df[column] = df[column].str.decode('utf-8')\n",
    "\n",
    "# Connection setup\n",
    "connection = None\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(user=user, host=host, database=database, password=password)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    for sas_file in sas_files:\n",
    "        # Convert SAS7BDAT to CSV\n",
    "        sas_filename = f\"/dsa/groups/casestudy2023su/team03/{sas_file}\"\n",
    "        csv_filename = f\"/dsa/groups/casestudy2023su/team03/{sas_file}.csv\"\n",
    "\n",
    "        df = pd.read_sas(sas_filename)\n",
    "        binary2StringLiteral(df)\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # We're dealing with the raw data that could be very messy - starting with _raw for clarity\n",
    "        table_name = sas_file.split(\".\")[0].lower() + \"_raw\"\n",
    "\n",
    "        # Check if table exists\n",
    "        check_table_query = f\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '{schema}' AND table_name = '{table_name}')\"\n",
    "        cursor.execute(check_table_query)\n",
    "        table_exists = cursor.fetchone()[0]\n",
    "\n",
    "        if table_exists:\n",
    "            print(\"Table already exists - blowing it away: {}\".format(table_name))\n",
    "            # Drop the table if it exists\n",
    "            drop_table_query = f\"DROP TABLE {schema}.{table_name}\"\n",
    "            cursor.execute(drop_table_query)\n",
    "            connection.commit()\n",
    "\n",
    "        # Create the table\n",
    "        print(\"Creating Table: {}\".format(table_name))\n",
    "        \n",
    "        if len(df.columns) >= 1600:\n",
    "            # If the number of columns is equal to or more than 1600, create a table with a single \"data_column\"\n",
    "            create_table_query = f\"CREATE TABLE {schema}.{table_name} (data_column text)\"\n",
    "            cursor.execute(create_table_query)\n",
    "        else:\n",
    "            # If the number of columns is less than 1600, create a table with all the columns from the CSV\n",
    "            columns = ', '.join([f'\"{col}\" {dtype2SQL[str(df[col].dtype)]}' for col in df.columns])\n",
    "            create_table_query = f\"CREATE TABLE {schema}.{table_name} ({columns})\"\n",
    "            cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "\n",
    "        # Grant necessary privileges to all users\n",
    "        grant_query = f\"GRANT ALL PRIVILEGES ON TABLE {schema}.{table_name} TO PUBLIC\"\n",
    "        cursor.execute(grant_query)\n",
    "        connection.commit()\n",
    "        \n",
    "        print(f\"Table {schema}.{table_name} created successfully.\")\n",
    "\n",
    "        # Turns out we don't have permissions on public to use copy\n",
    "        # copy_query = f\"COPY {schema}.{table_name}(data_column) FROM '{csv_filename}' DELIMITER ',' CSV\"\n",
    "\n",
    "        # Alernative is to read in the CSV file and insert the data\n",
    "        # A bit slower... but it works! \n",
    "        with open(csv_filename, 'r') as file:\n",
    "            if len(df.columns) >= 1600:\n",
    "                # We don't skip the header because we need that data if we're building another table. \n",
    "                # If the number of columns is equal to or more than 1600, read in each row but don't parse the columns\n",
    "                for line in file:\n",
    "                    data_column = line.strip()\n",
    "                    insert_query = f\"INSERT INTO {schema}.{table_name} (data_column) VALUES (%s)\"\n",
    "                    cursor.execute(insert_query, (data_column,))\n",
    "            else:\n",
    "                # Skip header row\n",
    "                next(file)\n",
    "                \n",
    "                # If the number of columns is less than 1600, insert the data row by row\n",
    "                for line in file:\n",
    "                    values = line.strip().split(',')\n",
    "                    values = [None if x == \"\" else x for x in values]\n",
    "                    insert_query = f\"INSERT INTO {schema}.{table_name} VALUES ({','.join(['%s']*len(values))})\"\n",
    "                    cursor.execute(insert_query, values)\n",
    "\n",
    "        connection.commit()\n",
    "\n",
    "        print(f\"File {sas_file} inserted successfully into table {schema}.{table_name}.\")\n",
    "\n",
    "    print(\"All files inserted successfully.\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"An error has occurred: {}\".format(err))\n",
    "\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "del password\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_verify\"></a>Verify successful insert\n",
    "\n",
    "Now we have the opportunity to see whether the inserts were succesful. \n",
    "1. Connect to the database again\n",
    "2. Select data from the B1AUG16_raw to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Password and hit enter :: ········\n",
      "data from B1AUG16 table\n",
      "1('B1HPQDR', 'text')\n",
      "2('B1TRA', 'real')\n",
      "3('B1TRC', 'real')\n",
      "4('B1TRD', 'real')\n",
      "5('B1ITA', 'real')\n",
      "6('B1ITC', 'real')\n",
      "7('B1ITD', 'real')\n",
      "8('B1FNA', 'real')\n",
      "9('B1FNC', 'real')\n",
      "10('B1FND', 'real')\n",
      "11('B1WDA', 'real')\n",
      "12('B1WDC', 'real')\n",
      "13('B1WDD', 'real')\n",
      "14('B1THA', 'real')\n",
      "15('B1THC', 'real')\n",
      "16('B1THD', 'real')\n",
      "17('B1HPADT', 'text')\n",
      "18('B1HPDATE', 'text')\n",
      "19('B1HPTYP', 'real')\n",
      "20('B1HPMODE', 'real')\n",
      "21('B1HPCOD', 'text')\n",
      "22('B1HPPRO', 'real')\n",
      "23('B1HPSIDE', 'text')\n",
      "24('B1HPID', 'text')\n",
      "25('SITE', 'text')\n",
      "26('ID', 'text')\n",
      "27('B1SPQDR', 'text')\n",
      "28('B1L1A', 'real')\n",
      "29('B1L1C', 'real')\n",
      "30('B1L1D', 'real')\n",
      "31('B1L2A', 'real')\n",
      "32('B1L2C', 'real')\n",
      "33('B1L2D', 'real')\n",
      "34('B1L3A', 'real')\n",
      "35('B1L3C', 'real')\n",
      "36('B1L3D', 'real')\n",
      "37('B1L4A', 'real')\n",
      "38('B1L4C', 'real')\n",
      "39('B1L4D', 'real')\n",
      "40('B1TLA', 'real')\n",
      "41('B1TLC', 'real')\n",
      "42('B1TLD', 'real')\n",
      "43('B1SPADT', 'text')\n",
      "44('B1SPDATE', 'text')\n",
      "45('B1SPTYP', 'real')\n",
      "46('B1SPMODE', 'real')\n",
      "47('B1SPCOD', 'text')\n",
      "48('B1SPPRO', 'real')\n",
      "49('B1SPID', 'text')\n",
      "50('B1WBQDR', 'text')\n",
      "51('B1TBA', 'real')\n",
      "52('B1TBC', 'real')\n",
      "53('B1TBD', 'real')\n",
      "54('B1SBA', 'real')\n",
      "55('B1SBC', 'real')\n",
      "56('B1SBD', 'real')\n",
      "57('B1HDA', 'real')\n",
      "58('B1HDC', 'real')\n",
      "59('B1HDD', 'real')\n",
      "60('B1LAA', 'real')\n",
      "61('B1LAC', 'real')\n",
      "62('B1LAD', 'real')\n",
      "63('B1RAA', 'real')\n",
      "64('B1RAC', 'real')\n",
      "65('B1RAD', 'real')\n",
      "66('B1LRA', 'real')\n",
      "67('B1LRC', 'real')\n",
      "68('B1LRD', 'real')\n",
      "69('B1RRA', 'real')\n",
      "70('B1RRC', 'real')\n",
      "71('B1RRD', 'real')\n",
      "72('B1TSA', 'real')\n",
      "73('B1TSC', 'real')\n",
      "74('B1TSD', 'real')\n",
      "75('B1LSA', 'real')\n",
      "76('B1LSC', 'real')\n",
      "77('B1LSD', 'real')\n",
      "78('B1PEA', 'real')\n",
      "79('B1PEC', 'real')\n",
      "80('B1PED', 'real')\n",
      "81('B1LLA', 'real')\n",
      "82('B1LLC', 'real')\n",
      "83('B1LLD', 'real')\n",
      "84('B1RLA', 'real')\n",
      "85('B1RLC', 'real')\n",
      "86('B1RLD', 'real')\n",
      "87('B1WBADT', 'text')\n",
      "88('B1WBDATE', 'text')\n",
      "89('B1WBTYP', 'real')\n",
      "90('B1WBMODE', 'real')\n",
      "91('B1WBCOD', 'text')\n",
      "92('B1WBPRO', 'real')\n",
      "93('B1BRF', 'real')\n",
      "94('B1HDF', 'real')\n",
      "95('B1HDE', 'real')\n",
      "96('B1HDM', 'real')\n",
      "97('B1HDP', 'real')\n",
      "98('B1LAF', 'real')\n",
      "99('B1LAE', 'real')\n",
      "100('B1LAM', 'real')\n",
      "101('B1LAP', 'real')\n",
      "102('B1RAF', 'real')\n",
      "103('B1RAE', 'real')\n",
      "104('B1RAM', 'real')\n",
      "105('B1RAP', 'real')\n",
      "106('B1TKF', 'real')\n",
      "107('B1TKE', 'real')\n",
      "108('B1TKM', 'real')\n",
      "109('B1TKP', 'real')\n",
      "110('B1LLF', 'real')\n",
      "111('B1LLE', 'real')\n",
      "112('B1LLM', 'real')\n",
      "113('B1LLP', 'real')\n",
      "114('B1RLF', 'real')\n",
      "115('B1RLE', 'real')\n",
      "116('B1RLM', 'real')\n",
      "117('B1RLP', 'real')\n",
      "118('B1SBF', 'real')\n",
      "119('B1SBE', 'real')\n",
      "120('B1SBM', 'real')\n",
      "121('B1SBP', 'real')\n",
      "122('B1TBF', 'real')\n",
      "123('B1TBE', 'real')\n",
      "124('B1TBM', 'real')\n",
      "125('B1TBP', 'real')\n",
      "126('B1TKC', 'real')\n",
      "127('B1TBL', 'real')\n",
      "128('B1HDL', 'real')\n",
      "129('B1LAL', 'real')\n",
      "130('B1RAL', 'real')\n",
      "131('B1TKL', 'real')\n",
      "132('B1LLL', 'real')\n",
      "133('B1RLL', 'real')\n",
      "134('B1SBL', 'real')\n",
      "135('B1WBID', 'text')\n",
      "136('B1HGT', 'real')\n",
      "137('B1TBMKG', 'real')\n",
      "138('B1TBCKG', 'real')\n",
      "139('B1TBFKG', 'real')\n",
      "140('B1TBLKG', 'real')\n",
      "141('B1SBMKG', 'real')\n",
      "142('B1SBCKG', 'real')\n",
      "143('B1SBFKG', 'real')\n",
      "144('B1SBLKG', 'real')\n",
      "145('B1LALKG', 'real')\n",
      "146('B1LAFKG', 'real')\n",
      "147('B1RALKG', 'real')\n",
      "148('B1RAFKG', 'real')\n",
      "149('B1LLLKG', 'real')\n",
      "150('B1LLFKG', 'real')\n",
      "151('B1RLLKG', 'real')\n",
      "152('B1RLFKG', 'real')\n",
      "153('B1TKLKG', 'real')\n",
      "154('B1TKFKG', 'real')\n",
      "155('B1ASM', 'real')\n",
      "156('B1OTHLKG', 'real')\n",
      "157('B1OTHFKG', 'real')\n",
      "158('B1TBLBMI', 'real')\n",
      "159('B1TBFBMI', 'real')\n",
      "160('B1ASMBMI', 'real')\n",
      "161('B1TKFBMI', 'real')\n",
      "162('B1OTHLBMI', 'real')\n",
      "163('B1OTHFBMI', 'real')\n",
      "164('VISIT', 'text')\n",
      "165('B1FNT', 'real')\n"
     ]
    }
   ],
   "source": [
    "# Time to hop back in and let's see how things are looking!\n",
    "password = getpass.getpass('Type Password and hit enter :: ')\n",
    "\n",
    "connection = psycopg2.connect(database = 'casestdysu23t03', \n",
    "                              user = 'dtfp3', \n",
    "                              host = 'pgsql.dsa.lan',\n",
    "                              password = password)\n",
    "del password\n",
    "\n",
    "\n",
    "# Query column information\n",
    "with connection, connection.cursor() as cursor:\n",
    "    cursor.execute('''SELECT column_name, data_type\n",
    "                    FROM information_schema.columns\n",
    "                    WHERE table_schema = 'public' AND \n",
    "                    table_name = 'b1aug16_raw';''')\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    print(\"data from B1AUG16 table\")\n",
    "    i = 0\n",
    "    for row in results:\n",
    "        i += 1\n",
    "        print(str(i) + str(row))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"data_wide\"></a>Working with wide data\n",
    "Since we are working with very wide data, we had to account for that when doing our insert in to Postgres.  The **fafeb23_raw** table we created with greater than 1600 columns was stored as a single column.  \n",
    "\n",
    "This is an exploratory option of selecting and then creating a new table. **fafeb23_fx** with only a subset of the columns.  Depending on our EDA and analysis, we may choose to refactor this code to select different columns and create additional tables.  The runs through similar steps to above:\n",
    "1. Droping the fafeb23_fx table if it already exists\n",
    "2. Take the first row of the fafeb23_raw table to capture all the column names\n",
    "3. Retain only the column names that we'll be using for building our new table\n",
    "    1. In the case, we're only retaining the id, site, and \"fx\" columns\n",
    "4. Create the new table\n",
    "5. Grant the privileges on the new table\n",
    "6. Insert the row of data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Connection' object has no attribute 'cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fde8ae982740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Creating a new table, fafeb23_fx, from the public.fafeb23_raw table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdrop_table_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"DROP TABLE {schema}.{new_table_name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Connection' object has no attribute 'cursor'"
     ]
    }
   ],
   "source": [
    "# Define the new table information\n",
    "schema = \"public\"\n",
    "new_table_name = 'fafeb23_fx'\n",
    "original_table_name = 'fafeb23_raw'\n",
    "\n",
    "# Creating a new table, fafeb23_fx, from the public.fafeb23_raw table\n",
    "with connection, connection.cursor() as cursor:\n",
    "\n",
    "    drop_table_query = f\"DROP TABLE {schema}.{new_table_name}\"\n",
    "    cursor.execute(drop_table_query)\n",
    "    connection.commit()\n",
    "\n",
    "    \n",
    "    # Retrieve the first row from the table\n",
    "    row_query = f\"SELECT * FROM {schema}.{original_table_name} LIMIT 1\"\n",
    "    cursor.execute(row_query)\n",
    "    first_row = cursor.fetchone()[0]\n",
    "    #print(first_row)\n",
    "    \n",
    "    # Specify the desired columns to keep\n",
    "    columns_to_keep = ['id', 'site']\n",
    "    columns_containing_fx = [col for col in first_row.split(',') if 'fx' in col.lower()]\n",
    "    #print(columns_containing_fx)\n",
    "    \n",
    "    # Convert column names to lowercase\n",
    "    columns_to_keep = [col.lower() for col in columns_to_keep]\n",
    "    columns_containing_fx = [col.lower() for col in columns_containing_fx]\n",
    "\n",
    "    # Generate the column names for the new table\n",
    "    column_names = columns_to_keep + columns_containing_fx\n",
    "    print(column_names)\n",
    "\n",
    "    column_definitions = ', '.join([f'\"{col}\" text' for col in column_names])\n",
    "\n",
    "    # Create the new table\n",
    "    create_table_query = f\"CREATE TABLE {new_table_name} ({column_definitions})\"\n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "\n",
    "    # Grant necessary privileges to all users\n",
    "    grant_query = f\"GRANT ALL PRIVILEGES ON TABLE public.fafeb23_fx TO PUBLIC\"\n",
    "    cursor.execute(grant_query)\n",
    "    connection.commit()\n",
    "    \n",
    "# Read the CSV file associated with the desired columns\n",
    "# Need to make them uppercase though...\n",
    "column_names = [col.upper() for col in column_names]\n",
    "\n",
    "csv_filename = f'/dsa/groups/casestudy2023su/team03/FAFEB23.SAS7BDAT.csv'\n",
    "df = pd.read_csv(csv_filename, usecols=column_names)\n",
    "\n",
    "# Insert data into the new table\n",
    "with connection, connection.cursor() as cursor:\n",
    "    # Iterate over the rows of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        values = [str(row[col]) for col in column_names]\n",
    "        insert_query = f\"INSERT INTO {new_table_name} ({', '.join(column_names)}) VALUES ({', '.join(['%s'] * len(column_names))})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "    connection.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
